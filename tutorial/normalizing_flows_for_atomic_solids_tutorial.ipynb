{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkzoVxJtnO4k"
      },
      "source": [
        "# Tutorial\n",
        "\n",
        "The goal of this tutorial colab is to train the normalizing flow model proposed in reference [1] on a small system comprising 8 particles of monatomic water (mW) in the cubic ice phase. \n",
        "\n",
        "All the relevant code is available on github [2] as supplemental material to the publication but some parts of the logic are missing here and need to be implemented for the model to train. In particular, in the functions `get_loss`, `get_eval_metrics`, and `get_normalised_target_log_probs` below you have to supply the missing code as specified in the *Todo* field of their docstrings.\n",
        "\n",
        "Training the model with the hyperparameters (config) below does not require any hardware accelerators and the model should reach an effective sample size (ESS) of about 10% on a public CPU kernel in under 10 minutes.\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "**References**\n",
        "\n",
        "[1] Wirnsberger, Papamakarios, Ibarz et al., *Normalizing flows for atomic solids*, Mach. Learn.: Sci. Technol. 3 025009 (2022), [link](https://iopscience.iop.org/article/10.1088/2632-2153/ac6b16).\n",
        "\n",
        "[2] Supplemental code for *Normalizing flows for atomic solids* on github: [deepmind/flows_for_atomic_solids](https://github.com/deepmind/flows_for_atomic_solids).\n",
        "\n",
        "[3] Jarzynski, *Targeted free energy perturbation*, Phys. Rev. E 65, 046122 (2002), [link](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.65.046122).\n",
        "\n",
        "[4] Wirnsberger, Ballard et al., *Targeted free energy estimation via learned mappings*, J. Chem. Phys. 153, 144112 (2020), [link](https://doi.org/10.1063/5.0018903).\n",
        "\n",
        "[5] Nicoli et al., *Asymptotically unbiased estimation of physical observables with neural samplers*, Phys. Rev. E 101, 023304 (2020), [link](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.101.023304).\n",
        "\n",
        "[6] Frenkel and Smit, *Understanding molecular simulation*, 2nd edition, San Diego (2002), [link](https://www.sciencedirect.com/book/9780122673511/understanding-molecular-simulation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1_Z-t37wcyP"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45u0W_hFwfte"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/deepmind/flows_for_atomic_solids.git\n",
        "!pip install -r flows_for_atomic_solids/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDMBM1658EWE"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Dict, Tuple, Union\n",
        "from absl import app\n",
        "from absl import flags\n",
        "import chex\n",
        "import distrax\n",
        "from flows_for_atomic_solids.experiments import monatomic_water_config\n",
        "from flows_for_atomic_solids.experiments import utils\n",
        "from flows_for_atomic_solids.utils import observable_utils as obs_utils\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
        "from matplotlib import rcParams\n",
        "import optax\n",
        "\n",
        "Array = chex.Array\n",
        "Numeric = Union[Array, float]\n",
        "\n",
        "rcParams.update({\n",
        "    'font.size': 16, 'xtick.labelsize': 16, 'ytick.labelsize': 16,\n",
        "    'legend.fontsize': 16, 'lines.linewidth': 3, 'axes.titlepad': 16,\n",
        "    'axes.labelpad': 16, 'axes.labelsize': 20,\n",
        "    'figure.figsize': [8.0, 6.0]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8L3Z-y3wnMG"
      },
      "source": [
        "## Model specification\n",
        "\n",
        "All relevant model and training hyperparameters, such as the number of flow layers, the batch size, the learning rate, the energy function, the thermodynamic state, etc., are defined in a `ConfigDict`. Training the model with the hyperparameters used in reference [1], see for example [monatomic_water_config.py](https://github.com/deepmind/flows_for_atomic_solids/blob/main/experiments/monatomic_water_config.py), would require multiple accelerators, such as GPUs. We therefore overwrite some of the parameters of the above config in order to reduce the model size and to optimise for quick training on a CPU. \n",
        "\n",
        "With the settings below, the model should train to about 10% ESS in less than ten minutes on a CPU. The model will still improve if you train it longer (by increasing `num_training_steps`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-AipMIcHIWY"
      },
      "outputs": [],
      "source": [
        "# Get the default config for 8-particle cubic ice. \n",
        "config = monatomic_water_config.get_config(num_particles=8, lattice='cubic')\n",
        "\n",
        "# Update specific hyperparameters for fast training. \n",
        "config.train.learning_rate=1e-3\n",
        "config.model.kwargs.bijector.kwargs.num_layers=4\n",
        "config.model.kwargs.bijector.kwargs.num_bins=16\n",
        "config.model.kwargs.bijector.kwargs.conditioner.kwargs.num_frequencies=2\n",
        "config.model.kwargs.bijector.kwargs.conditioner.kwargs.embedding_size=32\n",
        "config.test.batch_size=16384\n",
        "config.test.test_every=500\n",
        "\n",
        "# `state` is a dictionary that contains information about the thermodynamic \n",
        "# state, such as the number of particles, the inverse temperature, and the \n",
        "# extents of the simulation box.\n",
        "state = config.state\n",
        "\n",
        "# Defines the number of training steps (the number of parameters updates).\n",
        "num_training_steps=501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40kCVeO5o8w4"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "In this section, we define the training objective and a set of evaluation metrics that allows us to monitor the training progress. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzcOfrDlT642"
      },
      "source": [
        "### Training objective\n",
        "\n",
        "We train our model $q(x)$ to approximate the target Boltzmann distribution \n",
        "\n",
        "\\begin{equation*}\n",
        "  p(x) = \\frac{1}{Z} e^{-\\beta U(x)}\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        " by minimizing a Kullback&ndash;Leibler divergence as the loss function:\n",
        "\n",
        "\\begin{equation*}\n",
        "    D(q || p) = {\\langle{\\ln{ q(x)} - \\ln{p(x)}}\\rangle}_q = {\\langle{\\ln{ q(x)} + \\beta U(x)}\\rangle}_q + \\ln{Z}.  \n",
        "\\end{equation*}\n",
        "\n",
        "The last term, $\\ln Z$, is the logarithm of the normalizing constant and it can be ignored as its gradient with respect to the model parameters vanishes. The function $U(x)$ is a given energy function (here the mW potential) and $\\beta = (k_\\text{B} T)^{-1}$ is the inverse temperature with $k_\\text{B}$ being the Boltzmann constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRjrOfJMT7Ke"
      },
      "outputs": [],
      "source": [
        "def get_loss(model: distrax.Distribution, energy_fn: Callable[[Array], Array],\n",
        "             beta: Numeric, num_samples: int) -> Tuple[Array, Dict[str, Array]]:\n",
        "  \"\"\"Returns the loss and some additional metrics.\n",
        "\n",
        "  Args:\n",
        "    model: our model from which we can sample.\n",
        "    energy_fn: a function that takes a batch of samples and returns a batch of\n",
        "      energies.\n",
        "    beta: the inverse temperature.\n",
        "    num_samples: the number of samples to be used for computing the loss.\n",
        "\n",
        "  Returns:\n",
        "    The scalar loss and a dictionary containing energies, model log probs and \n",
        "    target log probs.\n",
        "\n",
        "  Todo: \n",
        "    Draw a batch of samples from the model and implement the quantities that \n",
        "    are currently set to zero.\n",
        "  \"\"\"\n",
        "  rng_key = hk.next_rng_key()\n",
        "\n",
        "  zeros = jnp.zeros(config.train.batch_size)\n",
        "  loss = 0.                # <ln q(x) + \\beta U(x)>\n",
        "  energy = zeros           # U(x)\n",
        "  model_log_prob = zeros   # ln q(x)\n",
        "  target_log_prob = zeros  # ln p(x)\n",
        "  \n",
        "  stats = {\n",
        "      'energy': energy,\n",
        "      'model_log_prob': model_log_prob,\n",
        "      'target_log_prob': target_log_prob\n",
        "  }\n",
        "  return loss, stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s28Wx8dsmKb_"
      },
      "source": [
        "### Evaluation metrics\n",
        "\n",
        "After every `config.test.test_every` training steps, we evaluate a set of metrics to monitor the training progress.\n",
        "<br/><br/>\n",
        "\n",
        "**Normalizing constant:**\n",
        "To estimate $\\ln Z$, we can use a targeted free energy estimator [3&ndash;5]. We first compute the forward work values\n",
        "\n",
        "\\begin{equation*}\n",
        "  \\beta \\Phi(x) = \\beta U(x) + \\ln{q(x)}\n",
        "\\end{equation*}\n",
        "\n",
        "and then the asymptotically unbiased estimate\n",
        "\n",
        "\\begin{equation*}\n",
        "\\ln{Z} = \\ln { \\langle{\\exp(-\\beta \\Phi(x)) \\rangle }}_q.\n",
        "\\end{equation*}\n",
        "<br/>\n",
        "\n",
        "**Expected energy:**\n",
        "We can compute an unbiased estimate of the potential energy, ${\\langle U \\rangle}$, via importance sampling\n",
        "\n",
        "\\begin{equation*}\n",
        "{\\langle U \\rangle} = \\frac{ \\sum_n w_n U(x_n)} {\\sum_n w_n},\n",
        "\\end{equation*}\n",
        "\n",
        "where $w_n = p^*(x_n)/q(x_n)$, $x_n \\sim q(x)$ and $p^*(x) = Z p(x)$.\n",
        "<br/><br/>\n",
        "\n",
        "**Effective sample size (ESS):**\n",
        "The effective samples size can be estimated as \n",
        "\n",
        "\\begin{equation*}\n",
        "  {\\text{ESS}} = \\frac{ {\\left( \\sum_n w_n \\right)}^2} {\\sum_n w_n^2}.\n",
        "\\end{equation*}\n",
        "<br/>\n",
        "\n",
        "**Helmholtz Free energy:**\n",
        "Knowing the normalizing constant, we can compute the Helmholtz free energy $F$ via the relation\n",
        "\n",
        "\\begin{equation*}\n",
        "e^{-\\beta F}  = \\frac{Z}{N! \\Lambda^{3N}},\n",
        "\\end{equation*}\n",
        "\n",
        "where $N$ is the number of particles in the system and\n",
        "$\\Lambda = 2.3925~\\overset{\\circ}{\\text{A}}$ is the thermal de Broglie wavelength, which we set to the same value as the $\\sigma$ parameter of the mW potential.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qOKZadwmKpd"
      },
      "outputs": [],
      "source": [
        "def get_eval_metrics(loss: Array, beta: Numeric, num_particles: int,\n",
        "                     energy: Array, model_log_prob: Array,\n",
        "                     target_log_prob: Array) -> Dict[str, Array]:\n",
        "  \"\"\"Returns the evaluation metrics.\n",
        "\n",
        "  Args:\n",
        "    loss: a scalar containing the loss.\n",
        "    beta: the inverse temperature.\n",
        "    num_particles: the number of particles.\n",
        "    energy: an array with shape [batch_size] containing energy values.\n",
        "    model_log_prob: an array with shape [batch_size] containing log probs \n",
        "      under the model.\n",
        "    target_log_prob: an array with shape [batch_size] containing log probs \n",
        "      under the target.\n",
        "\n",
        "  Returns:\n",
        "     A dictionary containing the evaluation metrics.\n",
        "\n",
        "  Todo: \n",
        "    Implement the metrics that are currently set to zero.\n",
        "  \"\"\"\n",
        "  energy_biased = 0.     # <U(x)>_q\n",
        "  energy_unbiased = 0.   # <U(x)>_p\n",
        "  ess = 0.               # ESS\n",
        "  logz = 0.              # log Z\n",
        "  beta_f = 0.            # \\beta F/N\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "      'energy_biased': energy_biased,\n",
        "      'energy_unbiased': energy_unbiased,\n",
        "      'ess': ess,\n",
        "      'logz': logz,\n",
        "      'beta_f': beta_f,\n",
        "  }\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc7YZbrnw0DO"
      },
      "source": [
        "### Training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA7D3DqY4z-5"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "  return config.model['constructor'](\n",
        "      num_particles=state.num_particles,\n",
        "      lower=state.lower,\n",
        "      upper=state.upper,\n",
        "      **config.model['kwargs'])\n",
        "\n",
        "def train(num_iterations: int):\n",
        "  energy_fn_train = config.train_energy.constructor(\n",
        "      **config.train_energy.kwargs)\n",
        "  energy_fn_test = config.test_energy.constructor(**config.test_energy.kwargs)\n",
        "  lr_schedule_fn = utils.get_lr_schedule(\n",
        "      config.train.learning_rate, config.train.learning_rate_decay_steps,\n",
        "      config.train.learning_rate_decay_factor)\n",
        "  optimizer = optax.chain(\n",
        "      optax.scale_by_adam(),\n",
        "      optax.scale_by_schedule(lr_schedule_fn),\n",
        "      optax.scale(-1))\n",
        "  if config.train.max_gradient_norm is not None:\n",
        "    optimizer = optax.chain(\n",
        "        optax.clip_by_global_norm(config.train.max_gradient_norm), optimizer)\n",
        "\n",
        "  def loss_fn():\n",
        "    \"\"\"Loss function for training.\"\"\"\n",
        "    model = create_model()\n",
        "    loss, stats = get_loss(\n",
        "        model=model,\n",
        "        energy_fn=energy_fn_train,\n",
        "        beta=state.beta,\n",
        "        num_samples=config.train.batch_size)\n",
        "    train_metrics = dict(loss=loss, energy_biased=stats['energy'].mean())\n",
        "    return loss, train_metrics\n",
        "\n",
        "  def print_formatted(mode, step, metrics):\n",
        "    \"\"\"Output the training progress with nice formatting.\"\"\"\n",
        "    print(f'{mode}[{step}]') \n",
        "    for k in sorted(metrics.keys()):\n",
        "      print(f'   {k:<20}: {metrics[k]:g}')\n",
        "    print('-' * 34)\n",
        "    \n",
        "  def eval_fn():\n",
        "    \"\"\"Evaluation function.\"\"\"\n",
        "    model = create_model()\n",
        "    loss, stats = get_loss(\n",
        "        model=model,\n",
        "        energy_fn=energy_fn_test,\n",
        "        beta=state.beta,\n",
        "        num_samples=config.test.batch_size)\n",
        "    return get_eval_metrics(loss=loss, beta=state.beta, \n",
        "                            num_particles=state.num_particles, **stats)\n",
        " \n",
        "  print('Initialising system.')\n",
        "  rng_key = jax.random.PRNGKey(config.train.seed)\n",
        "  init_fn, apply_fn = hk.transform(loss_fn)\n",
        "  _, apply_eval_fn = hk.transform(eval_fn)\n",
        "\n",
        "  rng_key, init_key = jax.random.split(rng_key)\n",
        "  params = init_fn(init_key)\n",
        "  opt_state = optimizer.init(params)\n",
        "\n",
        "  def _loss(params, rng):\n",
        "    loss, metrics = apply_fn(params, rng)\n",
        "    return loss, metrics\n",
        "\n",
        "  jitted_loss = jax.jit(jax.value_and_grad(_loss, has_aux=True))\n",
        "  jitted_eval = jax.jit(apply_eval_fn)\n",
        "  \n",
        "  step = 0\n",
        "  print('Beginning of training.')\n",
        "  while step < num_iterations:\n",
        "    rng_key, loss_key = jax.random.split(rng_key)\n",
        "    (_, metrics), g = jitted_loss(params, loss_key)\n",
        "\n",
        "    if (step % 50) == 0:\n",
        "      print_formatted('Train', step, metrics)\n",
        "\n",
        "    if (step % config.test.test_every) == 0:\n",
        "      rng_key, val_key = jax.random.split(rng_key)\n",
        "      metrics = jitted_eval(params, val_key)\n",
        "      print_formatted('Valid', step, metrics)\n",
        "     \n",
        "    # Update parameters.\n",
        "    updates, opt_state = optimizer.update(g, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    step += 1\n",
        "    \n",
        "  return params\n",
        "\n",
        "params = train(num_training_steps)\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c60UZvdIODA"
      },
      "source": [
        "Some reference values:\n",
        "- $\\langle U \\rangle_p \\approx -94.64~\\text{kcal/mol}$\n",
        "- $\\beta F/N \\approx -25.86$ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcxzfyJNxsQr"
      },
      "source": [
        "## Analyzing the trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GHKHp-21228"
      },
      "source": [
        "### Sampling from the model\n",
        "\n",
        "\n",
        "The flow model is a probability distribution $q$ that is related to the base density $b$ via a diffeomorphism $f$. We can sample from $q$ by first sampling $z$ from $b$ and then taking $x = f(z)$. The probability density of `x` is given by\n",
        "\n",
        "\\begin{equation}\n",
        "    q(x) = b(z)|\\det J_f(z)|^{-1},\n",
        "\\end{equation}\n",
        "\n",
        "where $J_f$ is the Jacobian of $f$. \n",
        "\n",
        "Here, $q$ and $b$ are implemented as `distrax.Distribution` and have a function `sample_and_log_prob` that returns a batch of samples with corresponding log probs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPVmqs0M13VP"
      },
      "outputs": [],
      "source": [
        "@hk.transform\n",
        "def sample_and_log_prob(num_samples: int):\n",
        "  \"\"\"Returns samples and log probs from the base and the trained model.\"\"\"\n",
        "  key = hk.next_rng_key()\n",
        "  model = create_model()\n",
        "  return (\n",
        "      model.sample_and_log_prob(seed=key, sample_shape=num_samples),\n",
        "      model._base_model._flow_model.distribution.sample_and_log_prob(\n",
        "          seed=key, sample_shape=num_samples))\n",
        "\n",
        "((model_samples, model_log_probs), \n",
        " (base_samples, base_log_probs)) = sample_and_log_prob.apply(params, jax.random.PRNGKey(42), 4096)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q22QrIULyJW3"
      },
      "source": [
        "### Estimating normalised target log probs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate\n",
        "\n",
        "\\begin{equation*}\n",
        "\\ln p(x) = -\\beta U(x) - \\ln Z,\n",
        "\\end{equation*}\n",
        "\n",
        "we need the exact value of $\\ln Z$, which is unknown. However, we have a targeted estimate, $\\widehat{\\ln Z}$, that we can use to compute approximately normalised target log probs,\n",
        "\n",
        "\\begin{equation*}\n",
        "\\ln \\hat p(x) = -\\beta U(x) - \\widehat{\\ln Z}.\n",
        "\\end{equation*}\n"
      ],
      "metadata": {
        "id": "hTenSxqi3fD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgSuArBQyUhF"
      },
      "outputs": [],
      "source": [
        "energy_fn = config.test_energy.constructor(**config.test_energy.kwargs)\n",
        "\n",
        "def get_normalised_target_log_probs(  \n",
        "    model_samples: Array,\n",
        "    model_log_probs: Array) -> Array:\n",
        "  \"\"\"Returns the (approximately) normalised target log probs.\n",
        "\n",
        "  Args:\n",
        "    model_samples: samples drawn from the model.\n",
        "    model_log_probs: model log probs for the `model_samples`.\n",
        " \n",
        "  Returns:\n",
        "    approximately normalised target log probs.\n",
        "\n",
        "  Todo: \n",
        "    Estimate the normalising constant and use it to estimate the normalised\n",
        "    target log probs.\n",
        "  \"\"\"\n",
        "  normalised_target_log_probs = jnp.zeros_like(model_log_probs)\n",
        "  return normalised_target_log_probs\n",
        "\n",
        "normalised_target_log_probs = get_normalised_target_log_probs(model_samples, model_log_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3968U-TbS6vP"
      },
      "source": [
        "### Plotting model vs. target log probs\n",
        "We evaluate both the model log probs $\\ln q$ and the approximate target log probs $\\ln \\hat p$ on the same batch of samples drawn from the model, and then plot them against each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njwzrkFQ2uxn"
      },
      "outputs": [],
      "source": [
        "xmin = min(normalised_target_log_probs.min(), model_log_probs.min(), base_log_probs.min())\n",
        "xmax = max(normalised_target_log_probs.max(), model_log_probs.max(), base_log_probs.max())\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "plt.xlim((xmin, xmax))\n",
        "plt.ylim((xmin, xmax))\n",
        "plt.scatter(normalised_target_log_probs, base_log_probs, c='blue', alpha=0.2, label='base')\n",
        "plt.scatter(normalised_target_log_probs, model_log_probs, c='red', alpha=0.2, label='model')\n",
        "plt.plot(x, x, linestyle='--', c='black')\n",
        "plt.xlabel(r'$\\ln \\hat{p}(x)$')\n",
        "plt.ylabel(r'$\\ln q(x)$')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb2Mv94xe83t"
      },
      "source": [
        "### Plotting the radial distribution function\n",
        "\n",
        "The radial distribution function $g(r)$ is the ratio of the average number density at a distance $r$ of an arbitrary reference atom and the average number density in an ideal gas at the same overall density (see reference [6] for more details)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reference values\n",
        "The reference values for $g(r)$ were obtained with Hamiltonian Monte Carlo and are stored in the array `reference_rdf`. "
      ],
      "metadata": {
        "id": "YEytVau8d_8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference_rdf = np.array([\n",
        "       [1.5499999e-02, 0.0000000e+00],\n",
        "       [4.6499986e-02, 0.0000000e+00],\n",
        "       [7.7499993e-02, 0.0000000e+00],\n",
        "       [1.0849999e-01, 0.0000000e+00],\n",
        "       [1.3949999e-01, 0.0000000e+00],\n",
        "       [1.7050001e-01, 0.0000000e+00],\n",
        "       [2.0149997e-01, 0.0000000e+00],\n",
        "       [2.3250003e-01, 0.0000000e+00],\n",
        "       [2.6350003e-01, 0.0000000e+00],\n",
        "       [2.9449993e-01, 0.0000000e+00],\n",
        "       [3.2549998e-01, 0.0000000e+00],\n",
        "       [3.5650003e-01, 0.0000000e+00],\n",
        "       [3.8749993e-01, 0.0000000e+00],\n",
        "       [4.1849995e-01, 0.0000000e+00],\n",
        "       [4.4950002e-01, 0.0000000e+00],\n",
        "       [4.8050013e-01, 0.0000000e+00],\n",
        "       [5.1150000e-01, 0.0000000e+00],\n",
        "       [5.4249990e-01, 0.0000000e+00],\n",
        "       [5.7349980e-01, 0.0000000e+00],\n",
        "       [6.0449988e-01, 0.0000000e+00],\n",
        "       [6.3549995e-01, 0.0000000e+00],\n",
        "       [6.6650003e-01, 0.0000000e+00],\n",
        "       [6.9749999e-01, 0.0000000e+00],\n",
        "       [7.2850013e-01, 0.0000000e+00],\n",
        "       [7.5949979e-01, 0.0000000e+00],\n",
        "       [7.9049981e-01, 0.0000000e+00],\n",
        "       [8.2149994e-01, 0.0000000e+00],\n",
        "       [8.5249996e-01, 0.0000000e+00],\n",
        "       [8.8349998e-01, 0.0000000e+00],\n",
        "       [9.1449994e-01, 0.0000000e+00],\n",
        "       [9.4549960e-01, 0.0000000e+00],\n",
        "       [9.7649974e-01, 0.0000000e+00],\n",
        "       [1.0074998e+00, 0.0000000e+00],\n",
        "       [1.0384998e+00, 0.0000000e+00],\n",
        "       [1.0695000e+00, 0.0000000e+00],\n",
        "       [1.1005000e+00, 0.0000000e+00],\n",
        "       [1.1315001e+00, 0.0000000e+00],\n",
        "       [1.1625001e+00, 0.0000000e+00],\n",
        "       [1.1934999e+00, 0.0000000e+00],\n",
        "       [1.2245001e+00, 0.0000000e+00],\n",
        "       [1.2555002e+00, 0.0000000e+00],\n",
        "       [1.2864996e+00, 0.0000000e+00],\n",
        "       [1.3174998e+00, 0.0000000e+00],\n",
        "       [1.3484998e+00, 0.0000000e+00],\n",
        "       [1.3794997e+00, 0.0000000e+00],\n",
        "       [1.4104997e+00, 0.0000000e+00],\n",
        "       [1.4414998e+00, 0.0000000e+00],\n",
        "       [1.4724998e+00, 0.0000000e+00],\n",
        "       [1.5035000e+00, 0.0000000e+00],\n",
        "       [1.5345000e+00, 0.0000000e+00],\n",
        "       [1.5655000e+00, 0.0000000e+00],\n",
        "       [1.5965002e+00, 0.0000000e+00],\n",
        "       [1.6275002e+00, 0.0000000e+00],\n",
        "       [1.6585003e+00, 0.0000000e+00],\n",
        "       [1.6894995e+00, 0.0000000e+00],\n",
        "       [1.7204996e+00, 0.0000000e+00],\n",
        "       [1.7514995e+00, 0.0000000e+00],\n",
        "       [1.7824997e+00, 0.0000000e+00],\n",
        "       [1.8134997e+00, 0.0000000e+00],\n",
        "       [1.8444999e+00, 0.0000000e+00],\n",
        "       [1.8755000e+00, 0.0000000e+00],\n",
        "       [1.9065002e+00, 0.0000000e+00],\n",
        "       [1.9374998e+00, 0.0000000e+00],\n",
        "       [1.9684998e+00, 0.0000000e+00],\n",
        "       [1.9995000e+00, 0.0000000e+00],\n",
        "       [2.0304999e+00, 0.0000000e+00],\n",
        "       [2.0615001e+00, 0.0000000e+00],\n",
        "       [2.0924993e+00, 0.0000000e+00],\n",
        "       [2.1235003e+00, 0.0000000e+00],\n",
        "       [2.1544995e+00, 0.0000000e+00],\n",
        "       [2.1854997e+00, 0.0000000e+00],\n",
        "       [2.2164996e+00, 4.4473531e-04],\n",
        "       [2.2474999e+00, 6.4883090e-04],\n",
        "       [2.2784998e+00, 3.1564615e-03],\n",
        "       [2.3095002e+00, 1.0650708e-02],\n",
        "       [2.3404999e+00, 2.8518641e-02],\n",
        "       [2.3714993e+00, 8.4110245e-02],\n",
        "       [2.4024999e+00, 1.9078535e-01],\n",
        "       [2.4334996e+00, 3.9773461e-01],\n",
        "       [2.4645002e+00, 7.6246047e-01],\n",
        "       [2.4954996e+00, 1.3214793e+00],\n",
        "       [2.5265002e+00, 2.0774045e+00],\n",
        "       [2.5574994e+00, 2.9314387e+00],\n",
        "       [2.5885000e+00, 3.8366539e+00],\n",
        "       [2.6194994e+00, 4.5187459e+00],\n",
        "       [2.6505001e+00, 4.9712882e+00],\n",
        "       [2.6814997e+00, 5.1065240e+00],\n",
        "       [2.7125003e+00, 4.8710790e+00],\n",
        "       [2.7434998e+00, 4.3332777e+00],\n",
        "       [2.7744992e+00, 3.6022041e+00],\n",
        "       [2.8054998e+00, 2.8767633e+00],\n",
        "       [2.8364992e+00, 2.1645274e+00],\n",
        "       [2.8675001e+00, 1.5309756e+00],\n",
        "       [2.8984995e+00, 1.0224787e+00],\n",
        "       [2.9295001e+00, 6.5660924e-01],\n",
        "       [2.9604993e+00, 4.1806403e-01],\n",
        "       [2.9915004e+00, 2.4293379e-01],\n",
        "       [3.0224993e+00, 1.4242552e-01],\n",
        "       [3.0535002e+00, 7.6981105e-02],\n",
        "       [3.0844996e+00, 4.6274789e-02]])"
      ],
      "metadata": {
        "id": "j3agETBsduKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparison"
      ],
      "metadata": {
        "id": "FWZrsLebeslu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWrM7iybe-s6"
      },
      "outputs": [],
      "source": [
        "box_length = config.test_energy.kwargs.box_length\n",
        "model_rdf = obs_utils.radial_distribution_function(model_samples, box_length, num_bins=100)\n",
        "base_rdf = obs_utils.radial_distribution_function(base_samples, box_length, num_bins=100)\n",
        "plt.plot(base_rdf[:, 0], base_rdf[:, 1], linestyle='--', c='blue', label='base')\n",
        "plt.plot(model_rdf[:, 0], model_rdf[:, 1], linestyle='-', c='red', label='model')\n",
        "plt.plot(reference_rdf[:, 0], reference_rdf[:, 1], linestyle='dotted', c='black', label='reference')\n",
        "plt.xlabel(r'$r$')\n",
        "plt.ylabel(r'$g(r)$')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "YEytVau8d_8y"
      ],
      "name": "Updates: normalizing_flows_for_atomic_solids_tutorial.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}