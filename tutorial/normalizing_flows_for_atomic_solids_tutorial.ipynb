{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkzoVxJtnO4k"
      },
      "source": [
        "# Tutorial\n",
        "\n",
        "The goal of this tutorial colab is to train the normalizing flow model proposed in reference [1] on a small system comprising 8 particles of monatomic water in the cubic ice phase. \n",
        "\n",
        "All the relevant code is available on github [2] as supplemental material to the publication but some parts of the logic are missing here and need to be implemented for the model to train.\n",
        "\n",
        "Training the model with the hyperparameters (config) below does not require any hardware accelerators and the model should reach an ESS of about 10% on a public CPU kernel in under 10 minutes.\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "**References**\n",
        "\n",
        "[1] Wirnsberger, Papamakarios, Ibarz et al., *Normalizing flows for atomic solids*, Mach. Learn.: Sci. Technol. 3 025009 (2022), [link](https://iopscience.iop.org/article/10.1088/2632-2153/ac6b16).\n",
        "\n",
        "[2] Supplemental code for *Normalizing flows for atomic solids* on github: [deepmind/flows_for_atomic_solids](https://github.com/deepmind/flows_for_atomic_solids).\n",
        "\n",
        "[3] Jarzynski, *Targeted free energy perturbation*, Phys. Rev. E 65, 046122 (2002), [link](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.65.046122).\n",
        "\n",
        "[4] Wirnsberger, Ballard et al., *Targeted free energy estimation via learned mappings*, J. Chem. Phys. 153, 144112 (2020), [link](https://doi.org/10.1063/5.0018903).\n",
        "\n",
        "[5] Nicoli et al., *Asymptotically unbiased estimation of physical observables with neural samplers*, Phys. Rev. E 101, 023304 (2020), [link](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.101.023304)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1_Z-t37wcyP"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45u0W_hFwfte"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/deepmind/flows_for_atomic_solids.git\n",
        "!pip install -r flows_for_atomic_solids/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDMBM1658EWE"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Dict, Tuple, Union\n",
        "from absl import app\n",
        "from absl import flags\n",
        "import chex\n",
        "import distrax\n",
        "from flows_for_atomic_solids.experiments import monatomic_water_config\n",
        "from flows_for_atomic_solids.experiments import utils\n",
        "from flows_for_atomic_solids.utils import observable_utils as obs_utils\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
        "from matplotlib import rcParams\n",
        "import optax\n",
        "\n",
        "Array = chex.Array\n",
        "Numeric = Union[Array, float]\n",
        "\n",
        "rcParams.update({\n",
        "    'font.size': 16, 'xtick.labelsize': 16, 'ytick.labelsize': 16,\n",
        "    'legend.fontsize': 16, 'lines.linewidth': 3, 'axes.titlepad': 16,\n",
        "    'axes.labelpad': 16, 'axes.labelsize': 20,\n",
        "    'figure.figsize': [8.0, 6.0]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8L3Z-y3wnMG"
      },
      "source": [
        "## Model specification\n",
        "\n",
        "We modify some hyperparameters in the default config from the paper to reduce the model size and to optimise for quick training on a CPU. With the settings below, the model should train to about 10% ESS in less than ten minutes on a CPU. The model will still improve if you train it for longer (by increasing `num_training_steps`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-AipMIcHIWY"
      },
      "outputs": [],
      "source": [
        "num_training_steps=501\n",
        "config = monatomic_water_config.get_config(num_particles=8, lattice='cubic')\n",
        "config.train.learning_rate=1e-3\n",
        "config.model.kwargs.bijector.kwargs.num_layers=4\n",
        "config.model.kwargs.bijector.kwargs.num_bins=16\n",
        "config.model.kwargs.bijector.kwargs.conditioner.kwargs.num_frequencies=2\n",
        "config.model.kwargs.bijector.kwargs.conditioner.kwargs.embedding_size=32\n",
        "config.test.batch_size=16384\n",
        "config.test.test_every=500\n",
        "state = config.state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40kCVeO5o8w4"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "In this section, we define the training objective and a set of evaluation metrics that allows us to monitor the training progress. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzcOfrDlT642"
      },
      "source": [
        "### Training objective\n",
        "\n",
        "We train our model $q(x)$ to approximate the target Boltzmann distribution \n",
        "\n",
        "\\begin{equation}\n",
        "  p(x) = \\frac{1}{Z} e^{-\\beta U(x)}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        " by minimizing a Kullback&ndash;Leibler divergence as the loss function:\n",
        "\n",
        "\\begin{equation}\n",
        "    D(q || p) = {\\langle{\\ln{ q(x)} - \\ln{p(x)}}\\rangle}_q = {\\langle{\\ln{ q(x)} + \\beta U(x)}\\rangle}_q + \\ln{Z}.  \n",
        "\\end{equation}\n",
        "\n",
        "The last term, $\\ln Z$, is the logarithm of the normalizing constant and it can therefore be ignored as its gradient with respect to the model parameters vanishes. The function $U(x)$ is a given energy function (here the mW potential) and $\\beta = (k_\\text{B} T)^{-1}$ is the inverse temperature with $k_\\text{B}$ being the Boltzmann constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRjrOfJMT7Ke"
      },
      "outputs": [],
      "source": [
        "def get_loss(model: distrax.Distribution, energy_fn: Callable[[Array], Array],\n",
        "             beta: Numeric, num_samples: int) -> Tuple[Array, Dict[str, Array]]:\n",
        "  \"\"\"Returns the loss and some additional metrics.\n",
        "\n",
        "  Args:\n",
        "    model: our model from which we can sample.\n",
        "    energy_fn: a function that takes a batch of samples and returns a batch of\n",
        "      energies.\n",
        "    beta: the inverse temperature.\n",
        "    num_samples: the number of samples to be used for computing the loss.\n",
        "\n",
        "  Returns:\n",
        "    The scalar loss and a dictionary containing energies, model log probs and \n",
        "    target log probs.\n",
        "\n",
        "  Todo: \n",
        "    Draw a batch of samples from the model and implement the quantities that \n",
        "    are currently set to zero.\n",
        "  \"\"\"\n",
        "  rng_key = hk.next_rng_key()\n",
        "\n",
        "  zeros = jnp.zeros(config.train.batch_size)\n",
        "  loss = 0.                # <ln q(x) + \\beta U(x)>\n",
        "  energy = zeros           # U(x)\n",
        "  model_log_prob = zeros   # ln q(x)\n",
        "  target_log_prob = zeros  # ln p(x)\n",
        "  \n",
        "  stats = {\n",
        "      'energy': energy,\n",
        "      'model_log_prob': model_log_prob,\n",
        "      'target_log_prob': target_log_prob\n",
        "  }\n",
        "  return loss, stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s28Wx8dsmKb_"
      },
      "source": [
        "### Evaluation metrics\n",
        "\n",
        "After every `config.test.test_every` training steps, we evaluate a set of metrics to monitor the training progress.\n",
        "<br/><br/>\n",
        "\n",
        "**Normalizing constant:**\n",
        "To estimate $\\ln Z$, we can use a targeted free energy estimator [3&ndash;5]. We first compute the forward work values\n",
        "\n",
        "\\begin{equation}\n",
        "  \\beta \\Phi(x) = \\beta U(x) + \\ln{q(x)}\n",
        "\\end{equation}\n",
        "\n",
        "and then the asymptotically unbiased estimate\n",
        "\n",
        "\\begin{equation}\n",
        "\\ln{Z} = \\ln { \\langle{\\exp(-\\beta \\Phi(x)) \\rangle }}_q.\n",
        "\\end{equation}\n",
        "<br/>\n",
        "\n",
        "**Expected energy:**\n",
        "We can compute an unbiased estimate of the potential energy, ${\\langle U \\rangle}$, via importance sampling\n",
        "\n",
        "\\begin{equation}\n",
        "{\\langle U \\rangle} = \\frac{ \\sum_n w_n U(x_n)} {\\sum_n w_n},\n",
        "\\end{equation}\n",
        "\n",
        "where $w_n = p^*(x_n)/q(x_n)$, $x_n \\sim q(x)$ and $p^*(x) = Z p(x)$.\n",
        "<br/><br/>\n",
        "\n",
        "**Effective sample size (ESS):**\n",
        "The effective samples size can be estimated as \n",
        "\n",
        "\\begin{equation}\n",
        "  {\\text{ESS}} = \\frac{ {\\left( \\sum_n w_n \\right)}^2} {\\sum_n w_n^2}.\n",
        "\\end{equation}\n",
        "<br/>\n",
        "\n",
        "**Helmholtz Free energy:**\n",
        "Knowing the normalizing constant, we can compute the Helmholtz free energy $F$ via the relation\n",
        "\n",
        "\\begin{equation}\n",
        "e^{-\\beta F}  = \\frac{ \\ln Z}{N! \\Lambda^{3N}},\n",
        "\\end{equation}\n",
        "\n",
        "where $N$ is the number of particles in the system and\n",
        "$\\Lambda = 2.3925~\\overset{\\circ}{\\text{A}}$ is the thermal de Broglie wavelength, which we set to the same value as the $\\sigma$ parameter of the mW potential.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qOKZadwmKpd"
      },
      "outputs": [],
      "source": [
        "def get_eval_metrics(loss: Array, energy: Array, model_log_prob: Array,\n",
        "                     target_log_prob: Array) -> Dict[str, Array]:\n",
        "  \"\"\"Returns the evaluation metrics.\n",
        "\n",
        "  Args:\n",
        "    loss: a scalar containing the loss.\n",
        "    energy: an array with shape [batch_size] containing energy values.\n",
        "    model_log_prob: an array with shape [batch_size] containing log probs \n",
        "      under the model.\n",
        "    target_log_prob: an array with shape [batch_size] containing log probs \n",
        "      under the target.\n",
        "\n",
        "  Returns:\n",
        "     A dictionary containing the evaluation metrics.\n",
        "\n",
        "  Todo: \n",
        "    Implement the metrics that are currently set to zero.\n",
        "  \"\"\"\n",
        "  energy_biased = 0.     # <U(x)>_q\n",
        "  energy_unbiased = 0.   # <U(x)>_p\n",
        "  ess = 0.               # ESS\n",
        "  logz = 0.              # log Z\n",
        "  beta_f = 0.            # \\beta F/N\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "      'energy_biased': energy_biased,\n",
        "      'energy_unbiased': energy_unbiased,\n",
        "      'ess': ess,\n",
        "      'logz': logz,\n",
        "      'beta_f': beta_f,\n",
        "  }\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc7YZbrnw0DO"
      },
      "source": [
        "### Training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OA7D3DqY4z-5"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "  return config.model['constructor'](\n",
        "      num_particles=state.num_particles,\n",
        "      lower=state.lower,\n",
        "      upper=state.upper,\n",
        "      **config.model['kwargs'])\n",
        "\n",
        "def train(num_iterations: int):\n",
        "  energy_fn_train = config.train_energy.constructor(\n",
        "      **config.train_energy.kwargs)\n",
        "  energy_fn_test = config.test_energy.constructor(**config.test_energy.kwargs)\n",
        "  lr_schedule_fn = utils.get_lr_schedule(\n",
        "      config.train.learning_rate, config.train.learning_rate_decay_steps,\n",
        "      config.train.learning_rate_decay_factor)\n",
        "  optimizer = optax.chain(\n",
        "      optax.scale_by_adam(),\n",
        "      optax.scale_by_schedule(lr_schedule_fn),\n",
        "      optax.scale(-1))\n",
        "  if config.train.max_gradient_norm is not None:\n",
        "    optimizer = optax.chain(\n",
        "        optax.clip_by_global_norm(config.train.max_gradient_norm), optimizer)\n",
        "\n",
        "  def loss_fn():\n",
        "    \"\"\"Loss function for training.\"\"\"\n",
        "    model = create_model()\n",
        "    loss, stats = get_loss(\n",
        "        model=model,\n",
        "        energy_fn=energy_fn_train,\n",
        "        beta=state.beta,\n",
        "        num_samples=config.train.batch_size)\n",
        "    train_metrics = dict(loss=loss, energy_biased=stats['energy'].mean())\n",
        "    return loss, train_metrics\n",
        "\n",
        "  def eval_fn():\n",
        "    \"\"\"Evaluation function.\"\"\"\n",
        "    model = create_model()\n",
        "    loss, stats = get_loss(\n",
        "        model=model,\n",
        "        energy_fn=energy_fn_test,\n",
        "        beta=state.beta,\n",
        "        num_samples=config.test.batch_size)\n",
        "    return get_eval_metrics(loss, **stats)\n",
        " \n",
        "  print('Initialising system.')\n",
        "  rng_key = jax.random.PRNGKey(config.train.seed)\n",
        "  init_fn, apply_fn = hk.transform(loss_fn)\n",
        "  _, apply_eval_fn = hk.transform(eval_fn)\n",
        "\n",
        "  rng_key, init_key = jax.random.split(rng_key)\n",
        "  params = init_fn(init_key)\n",
        "  opt_state = optimizer.init(params)\n",
        "\n",
        "  def _loss(params, rng):\n",
        "    loss, metrics = apply_fn(params, rng)\n",
        "    return loss, metrics\n",
        "\n",
        "  jitted_loss = jax.jit(jax.value_and_grad(_loss, has_aux=True))\n",
        "  jitted_eval = jax.jit(apply_eval_fn)\n",
        "  \n",
        "  step = 0\n",
        "  print('Beginning of training.')\n",
        "  while step < num_iterations:\n",
        "    rng_key, loss_key = jax.random.split(rng_key)\n",
        "    (_, metrics), g = jitted_loss(params, loss_key)\n",
        "\n",
        "    if (step % 50) == 0:\n",
        "      print(f'Train[{step}]: {metrics}')\n",
        "\n",
        "    if (step % config.test.test_every) == 0:\n",
        "      rng_key, val_key = jax.random.split(rng_key)\n",
        "      metrics = jitted_eval(params, val_key)\n",
        "      print(f'Valid[{step}]: {metrics}')\n",
        "\n",
        "    # Update parameters.\n",
        "    updates, opt_state = optimizer.update(g, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    step += 1\n",
        "    \n",
        "  return params\n",
        "\n",
        "params = train(num_training_steps)\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c60UZvdIODA"
      },
      "source": [
        "Some reference values:\n",
        "- $\\langle U \\rangle_p \\approx -94.64~\\text{kcal/mol}$\n",
        "- $\\beta F/N \\approx -25.86$ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcxzfyJNxsQr"
      },
      "source": [
        "## Analyzing the trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GHKHp-21228"
      },
      "source": [
        "### Sampling from the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oPVmqs0M13VP"
      },
      "outputs": [],
      "source": [
        "@hk.transform\n",
        "def sample_and_log_prob(num_samples: int):\n",
        "  \"\"\"Returns samples and log probs from the base and the trained model.\"\"\"\n",
        "  key = hk.next_rng_key()\n",
        "  model = create_model()\n",
        "  return (\n",
        "      model.sample_and_log_prob(seed=key, sample_shape=num_samples),\n",
        "      model._base_model._flow_model.distribution.sample_and_log_prob(\n",
        "          seed=key, sample_shape=num_samples))\n",
        "\n",
        "((model_samples, model_log_probs), \n",
        " (base_samples, base_log_probs)) = sample_and_log_prob.apply(params, jax.random.PRNGKey(42), 4096)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q22QrIULyJW3"
      },
      "source": [
        "### Estimating normalised target log probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZgSuArBQyUhF"
      },
      "outputs": [],
      "source": [
        "energy_fn = config.test_energy.constructor(**config.test_energy.kwargs)\n",
        "\n",
        "def get_normalised_target_log_probs(  \n",
        "    model_samples: Array,\n",
        "    model_log_probs: Array) -> Array:\n",
        "  \"\"\"Returns the (approximately) normalised target log probs.\n",
        "\n",
        "  Args:\n",
        "    model_samples: samples drawn from the model.\n",
        "    model_log_probs: model log probs for the `model_samples`.\n",
        " \n",
        "  Returns:\n",
        "    approximately normalised target log probs.\n",
        "\n",
        "  Todo: \n",
        "    Estimate the normalising constant and use it to estimate the normalised\n",
        "    target log probs: ln p(x) = -\\beta U(x) - ln Z.\n",
        "  \"\"\"\n",
        "  normalised_target_log_probs = jnp.zeros_like(model_log_probs)\n",
        "  return normalised_target_log_probs\n",
        "\n",
        "normalised_target_log_probs = get_normalised_target_log_probs(model_samples, model_log_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3968U-TbS6vP"
      },
      "source": [
        "### Plotting model vs. target log probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "njwzrkFQ2uxn"
      },
      "outputs": [],
      "source": [
        "xmin = min(normalised_target_log_probs.min(), model_log_probs.min(), base_log_probs.min())\n",
        "xmax = max(normalised_target_log_probs.max(), model_log_probs.max(), base_log_probs.max())\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "plt.xlim((xmin, xmax))\n",
        "plt.ylim((xmin, xmax))\n",
        "plt.scatter(normalised_target_log_probs, base_log_probs, c='blue', alpha=0.2, label='base')\n",
        "plt.scatter(normalised_target_log_probs, model_log_probs, c='red', alpha=0.2, label='model')\n",
        "plt.plot(x, x, linestyle='--', c='black')\n",
        "plt.xlabel(r'$\\ln \\hat{p}(x)$')\n",
        "plt.ylabel(r'$\\ln q(x)$')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb2Mv94xe83t"
      },
      "source": [
        "### Plotting the radial distribution function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PWrM7iybe-s6"
      },
      "outputs": [],
      "source": [
        "box_length = config.test_energy.kwargs.box_length\n",
        "model_rdf = obs_utils.radial_distribution_function(model_samples, box_length, num_bins=100)\n",
        "base_rdf = obs_utils.radial_distribution_function(base_samples, box_length, num_bins=100)\n",
        "plt.plot(base_rdf[:, 0], base_rdf[:, 1], linestyle='--', c='blue', label='base')\n",
        "plt.plot(model_rdf[:, 0], model_rdf[:, 1], linestyle='-', c='red', label='model')\n",
        "plt.xlabel(r'$r$')\n",
        "plt.ylabel(r'$g(r)$')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Normalizing_flows_for_atomic_solids--Tutorial.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}